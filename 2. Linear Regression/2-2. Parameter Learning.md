# 2. Linear Regression- 2) Parameter Learning(Gradient Descent)

## Gradient Descent
- hypothesis function의 최적의 parameter를 찾는 방법
- Gradient descent
	- cost function을 최소화하기 위해 이용할 수 있는 방법 중 하나
	- 각종 optimization에 이용되는 일반적인 방법


<img src="https://user-images.githubusercontent.com/46768752/77851666-52f6b100-7215-11ea-96dc-a60dc7e5b9d5.png"
width="700" height="150">

## Gradient Descent Algorithm

<img src="https://user-images.githubusercontent.com/46768752/77851674-57bb6500-7215-11ea-9db5-66b9308b78de.png"
width="700" height="530">


## Intuition

<img src="https://user-images.githubusercontent.com/46768752/78039704-6aff3980-73a9-11ea-9ff4-78aa54070fd4.png"
width="700" height="800">

<img src="https://user-images.githubusercontent.com/46768752/78039714-6e92c080-73a9-11ea-9f68-fa1196998c00.png"
width="700" height="500">


## Gradient Descent for Linear Regression
<img src="https://user-images.githubusercontent.com/46768752/78039733-73577480-73a9-11ea-9bde-6825aa63d28e.png"
width="700" height="550">


## Algorithm

<img src="https://user-images.githubusercontent.com/46768752/78039746-76eafb80-73a9-11ea-8c23-e2252c38b6b5.png"
width="700" height="350">



- 처음에는, 임의로 설정한 parameter에서 출발하여 iteration이 거듭될수록 점점 정확한 hypothesis가 됨.
- Linear regression cost function은 convex이므로 항상 global optima에 수렴

## Batch Gradient Descent
- "Batch": Gradient descent의 매 단계에서 모든 training example을 사용



```
< 영어단어 정리 >
- Gradient Descent: 경사 하강법
- optimization: 최적화

```